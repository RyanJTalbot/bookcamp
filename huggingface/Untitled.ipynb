{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ddc6762",
   "metadata": {},
   "source": [
    "We're going to use the wikitext (link) dataset with the distilbert-base-cased (link) model checkpoint.\n",
    "\n",
    "Start by loading the wikitext-2-raw-v1 version of that dataset, and take the 11th example (index 10) of the train split.\n",
    "We'll tokenize this using the appropriate tokenizer, and we'll mask the sixth token (index 5) the sequence.\n",
    "\n",
    "When using the distilbert-base-cased checkpoint to unmask that (sixth token, index 5) token, what is the most probable predicted token (please provide the decoded token, and not the ID)?\n",
    "\n",
    "Tips:\n",
    "- You might find the transformers docs (link) useful.\n",
    "- You might find the datasets docs (link) useful.\n",
    "- You might also be interested in the Hugging Face course (link).\n",
    "\n",
    "\n",
    "https://huggingface.co/spaces/internships/internships-2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37bad14",
   "metadata": {},
   "source": [
    "Tokenization is the process of breaking a string of text into individual words, phrases or other meaningful elements, known as tokens. In this case, the appropriate tokenizer for the distilbert-base-cased model checkpoint is the DistilBertTokenizer, which is trained to tokenize text in a way that is consistent with the way the model was trained on.\n",
    "\n",
    "Masking is a technique used in transformer-based models like BERT to replace a token in the input text with a special token, [MASK], during the training process. In this case, the sixth token (index 5) in the sequence will be replaced with the [MASK] token. This is done so that the model can learn to predict the original token, which is now hidden. The model is trained to predict the original token based on the context provided by the other tokens in the sequence.\n",
    "\n",
    "In summary, tokenization is the process of breaking the text into individual tokens and masking is the process of hiding a token in the input sequence to be predicted by the model during the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f23c5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseUrl = \"https://datasets-server.huggingface.co/first-rows?dataset=wikitext&config=wikitext-2-raw-v1&split=train\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b95eca11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://datasets-server.huggingface.co/first-rows?dataset=wikitext&config=wikitext-2-raw-v1&split=train'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseUrl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45d04abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "with open('dataset_infos.json', encoding='utf-8-sig') as f_input:\n",
    "    df = pd.read_json(f_input)\n",
    "\n",
    "df.to_csv('dataset_infos.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5ef1984",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset_infos.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8fd5978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 17 entries, 0 to 16\n",
      "Data columns (total 4 columns):\n",
      " #   Column               Non-Null Count  Dtype \n",
      "---  ------               --------------  ----- \n",
      " 0   wikitext-103-v1      13 non-null     object\n",
      " 1   wikitext-2-v1        13 non-null     object\n",
      " 2   wikitext-103-raw-v1  13 non-null     object\n",
      " 3   wikitext-2-raw-v1    13 non-null     object\n",
      "dtypes: object(4)\n",
      "memory usage: 672.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bf54634",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      The WikiText language modeling dataset is a c...\n",
       "1     @misc{merity2016pointer,\\n      title={Pointer...\n",
       "2     https://blog.einstein.ai/the-wikitext-long-ter...\n",
       "3     Creative Commons Attribution-ShareAlike 4.0 In...\n",
       "4     {'text': {'dtype': 'string', 'id': None, '_typ...\n",
       "5                                                   NaN\n",
       "6                                                   NaN\n",
       "7                                                   NaN\n",
       "8                                              wikitext\n",
       "9                                     wikitext-2-raw-v1\n",
       "10    {'version_str': '1.0.0', 'description': None, ...\n",
       "11    {'test': {'name': 'test', 'num_bytes': 1305092...\n",
       "12    {'https://s3.amazonaws.com/research.metamind.i...\n",
       "13                                              4721645\n",
       "14                                                  NaN\n",
       "15                                             13526117\n",
       "16                                             18247762\n",
       "Name: wikitext-2-raw-v1, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['wikitext-2-raw-v1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08ab5a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# from transformers import pipeline, set_seed\n",
    "# generator = pipeline('text-generation', model='gpt2')\n",
    "# generator(\"Hello, I like to play cricket,\", max_length=60, num_return_sequences=7)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "879fe766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nlp in /Users/ryantalbot/opt/anaconda3/lib/python3.9/site-packages (0.4.0)\n",
      "Requirement already satisfied: xxhash in /Users/ryantalbot/opt/anaconda3/lib/python3.9/site-packages (from nlp) (3.2.0)\n",
      "Requirement already satisfied: dill in /Users/ryantalbot/opt/anaconda3/lib/python3.9/site-packages (from nlp) (0.3.5.1)\n",
      "Requirement already satisfied: numpy in /Users/ryantalbot/opt/anaconda3/lib/python3.9/site-packages (from nlp) (1.21.5)\n",
      "Requirement already satisfied: pyarrow>=0.16.0 in /Users/ryantalbot/opt/anaconda3/lib/python3.9/site-packages (from nlp) (6.0.0)\n",
      "Requirement already satisfied: filelock in /Users/ryantalbot/opt/anaconda3/lib/python3.9/site-packages (from nlp) (3.6.0)\n",
      "Requirement already satisfied: pandas in /Users/ryantalbot/opt/anaconda3/lib/python3.9/site-packages (from nlp) (1.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/ryantalbot/opt/anaconda3/lib/python3.9/site-packages (from nlp) (4.64.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/ryantalbot/opt/anaconda3/lib/python3.9/site-packages (from nlp) (2.28.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ryantalbot/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->nlp) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ryantalbot/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->nlp) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/ryantalbot/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->nlp) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/ryantalbot/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->nlp) (2.0.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/ryantalbot/opt/anaconda3/lib/python3.9/site-packages (from pandas->nlp) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/ryantalbot/opt/anaconda3/lib/python3.9/site-packages (from pandas->nlp) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/ryantalbot/opt/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->nlp) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65dc7243",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-27 21:58:14.018584: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "790c44e499e5400c9a1ab0807aab5b41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/8.14k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ceda56d85714471a067c6d8e1fb6fdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/5.81k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset wikitext/wikitext-2-raw-v1 (download: 4.50 MiB, generated: 12.91 MiB, post-processed: Unknown sizetotal: 17.41 MiB) to /Users/ryantalbot/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/8e456126357b4411737ead54576f99321fc077a0d4b64e4a724ab3454ba5b730...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "463508cc07344bf9a8d98e06859e3ddd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/4.72M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset wikitext downloaded and prepared to /Users/ryantalbot/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/8e456126357b4411737ead54576f99321fc077a0d4b64e4a724ab3454ba5b730. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b341e500b315416f9058bea1317d2a3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/411 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "307b180af6ef4249b3cfe59284a790fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/251M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31a123a3dc464c268018644bb6a4a719",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/208k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deff9251c8cf42649189973c8b494a08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  The game 's battle system , the BliTZ system , is carried over directly from Valkyira Chronicles . During missions , players select each unit using a top @-@ down perspective of the battlefield map : once a character is selected , the player moves the character around the battlefield in third @-@ person . A character can only act once per @-@ turn , but characters can be granted multiple turns at the expense of other characters ' turns . Each character has a field and distance of movement limited by their Action Gauge . Up to nine characters can be assigned to a single mission . During gameplay , characters will call out if something happens to them , such as their health points ( HP ) getting low or being knocked out by enemy attacks . Each character has specific \" Potentials \" , skills unique to each character . They are divided into \" Personal Potential \" , which are innate skills that remain unaltered unless otherwise dictated by the story and can either help or impede a character , and \" Battle Potentials \" , which are grown throughout the game and always grant boons to a character . To learn Battle Potentials , each character has a unique \" Masters Table \" , a grid @-@ based skill table that can be used to acquire and link different skills . Characters also have Special Abilities that grant them temporary boosts on the battlefield : Kurt can activate \" Direct Command \" and move around the battlefield without depleting his Action Point gauge , the character Reila can shift into her \" Valkyria Form \" and become invincible , while Imca can target multiple enemy units with her heavy weapon . \n",
      "\n",
      "Predicted token: े\n",
      "Decoded input: The game's े system, the BliTZ system, is carried over directly from Valkyira Chronicles. During missions, players select each unit using a top @ - @ down perspective of the battlefield map : once a character is selected, the player moves the character around the battlefield in third @ - @ person. A character can only act once per @ - @ turn, but characters can be granted multiple turns at the expense of other characters'turns. Each character has a field and distance of movement limited by their Action Gauge. Up to nine characters can be assigned to a single mission. During gameplay, characters will call out if something happens to them, such as their health points ( HP ) getting low or being knocked out by enemy attacks. Each character has specific \" Potentials \", skills unique to each character. They are divided into \" Personal Potential \", which are innate skills that remain unaltered unless otherwise dictated by the story and can either help or impede a character, and \" Battle Potentials \", which are grown throughout the game and always grant boons to a character. To learn Battle Potentials, each character has a unique \" Masters Table \", a grid @ - @ based skill table that can be used to acquire and link different skills. Characters also have Special Abilities that grant them temporary boosts on the battlefield : Kurt can activate \" Direct Command \" and move around the battlefield without depleting his Action Point gauge, the character Reila can shift into her \" Valkyria Form \" and become invincible, while Imca can target multiple enemy units with her heavy weapon.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import nlp\n",
    "\n",
    "# Load the Wikitext-2 dataset\n",
    "dataset = nlp.load_dataset('wikitext', 'wikitext-2-raw-v1')\n",
    "\n",
    "# Get the 11th example (index 10) of the train split\n",
    "example = dataset['train'][10]\n",
    "\n",
    "# Load the DistilBERT model and tokenizer\n",
    "model = transformers.DistilBertModel.from_pretrained('distilbert-base-cased')\n",
    "tokenizer = transformers.DistilBertTokenizer.from_pretrained('distilbert-base-cased')\n",
    "\n",
    "# Tokenize the example\n",
    "input_ids = tokenizer.encode(example['text'], return_tensors='pt')\n",
    "\n",
    "# Mask the sixth token (index 5) in the sequence\n",
    "masked_input_ids = input_ids.clone()\n",
    "masked_input_ids[:, 5] = tokenizer.mask_token_id\n",
    "\n",
    "# Use the model to predict the most probable token for the masked token\n",
    "output = model(masked_input_ids)[0]\n",
    "prediction_scores, prediction_indexes = output[:, 5, :].max(dim=-1)\n",
    "# prediction_scores, prediction_indexes = output[:, 5, :].max(dim=-1)\n",
    "\n",
    "# Decode the predicted token ID to obtain the actual token\n",
    "predicted_token = tokenizer.decode(prediction_indexes, skip_special_tokens=True)\n",
    "\n",
    "# Replace the masked token with the predicted token in the input sequence\n",
    "decoded_input_ids = input_ids.squeeze().tolist()\n",
    "decoded_input_ids[5] = prediction_indexes.item()\n",
    "decoded_input = tokenizer.decode(decoded_input_ids, skip_special_tokens=True)\n",
    "\n",
    "print(f'Input: {example[\"text\"]}')\n",
    "print(f'Predicted token: {predicted_token}')\n",
    "print(f'Decoded input: {decoded_input}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6b7ddc9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/ryantalbot/Desktop/bookcamp/huggingface',\n",
       " '/Users/ryantalbot/opt/anaconda3/envs/tf2/lib/python39.zip',\n",
       " '/Users/ryantalbot/opt/anaconda3/envs/tf2/lib/python3.9',\n",
       " '/Users/ryantalbot/opt/anaconda3/envs/tf2/lib/python3.9/lib-dynload',\n",
       " '',\n",
       " '/Users/ryantalbot/.local/lib/python3.9/site-packages',\n",
       " '/Users/ryantalbot/opt/anaconda3/envs/tf2/lib/python3.9/site-packages',\n",
       " '/Users/ryantalbot/.local/lib/python3.9/site-packages/IPython/extensions',\n",
       " '/Users/ryantalbot/.ipython']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import sys\n",
    "# sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b03226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# /Users/yufeng/anaconda3/envs/py33/bin/python -m pip install plotly\n",
    "\n",
    "/Users/ryantalbot/opt/anaconda3/envs/tf2/bin/python3 -m pip install nlp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
