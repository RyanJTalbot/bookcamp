{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as se\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report,plot_confusion_matrix\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import SelectKBest,f_regression,f_classif\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from imblearn.combine import SMOTEENN\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from catboost import CatBoostClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Fetch\n",
    " Pandas is an open-source, BSD-licensed library providing high-performance,easy-to-use data manipulation and data analysis tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Fetch\n",
    "# file=''\n",
    "df=pd.read_csv('encoded_df.csv')\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    " It is the process of reducing the number of input variables when developing a predictive model.Used to reduce the number of input variables to reduce the computational cost of modelling and,in some cases,to improve the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selected Columns\n",
    "features=['LIST OF FEATURES/COLUMN NAMES']\n",
    "target='TARGET COLUMN NAME'\n",
    "# X & Y\n",
    "X=df[features]\n",
    "Y=df[target]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Encoding\n",
    " Converting the string classes data in the datasets by encoding them to integer either using OneHotEncoding or LabelEncoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling AlphaNumeric Features\n",
    "X=pd.get_dummies(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Matrix\n",
    " In order to check the correlation between the features, we will plot a correlation matrix. It is effective in summarizing a large amount of data where the goal is to see patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax = plt.subplots(figsize=(18, 18))\n",
    "matrix = np.triu(X.corr())\n",
    "se.heatmap(X.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax, mask=matrix)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-colinearity Test\n",
    " Dropping Highly Correlated Features to due similar features distributions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropHighCorrelationFeatures(X):\n",
    "        cor_matrix = X.corr()\n",
    "        upper_tri = cor_matrix.where(np.triu(np.ones(cor_matrix.shape),k=1).astype(np.bool))\n",
    "        to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.95)]\n",
    "        if to_drop!=[]: return X.drop(to_drop, axis=1)\n",
    "        else: return X\n",
    "X=dropHighCorrelationFeatures(X)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Feature Selection\n",
    " selecting 'n' best feature on the basis of ANOVA or Univariate Linear Regression Test. where ANOVA is used for Classification problem and Univariate Linear Regression for Regression problems\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_feature_importance(X,Y,score_func):\n",
    "    fit = SelectKBest(score_func=score_func, k=X.shape[1]).fit(X,Y)\n",
    "    dfscores,dfcolumns = pd.DataFrame(fit.scores_),pd.DataFrame(X.columns)\n",
    "    df = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "    df.columns = ['features','Score'] \n",
    "    df['Score']=MinMaxScaler().fit_transform(np.array(df['Score']).reshape(-1,1))\n",
    "    result=dict(df.values)\n",
    "    val=dict(sorted(result.items(), key=lambda item: item[1],reverse=False))\n",
    "    keylist=[]\n",
    "    for key, value in val.items():\n",
    "        if value < 0.01: keylist.append(key)\n",
    "    X=X.drop(keylist,axis=1)\n",
    "    plt.figure(figsize = (12, 6))\n",
    "    plt.barh(range(len(val)), list(val.values()), align='center')\n",
    "    plt.yticks(range(len(val)),list(val.keys()))\n",
    "    plt.xlabel(\"Importance\")\n",
    "    plt.ylabel(\"Feature\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return X\n",
    "X=get_feature_importance(X,Y,score_func=f_classif)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Rescaling\n",
    " Feature scaling or Data scaling is a method used to normalize the range of independent variables or features of data. In data processing, it is also known as data normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=X.columns\n",
    "X=RobustScaler().fit_transform(X)\n",
    "X=pd.DataFrame(data = X,columns = columns)\n",
    "X.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train & Test\n",
    " The train-test split is a procedure for evaluating the performance of an algorithm.The procedure involves taking a dataset and dividing it into two subsets.The first subset is utilized to fit/train the model.The second subset is used for prediction.The main motive is to estimate the performance of the model on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data split for training and testing\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.2,random_state=123)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Balancing\n",
    " SMOTEENN method combines the SMOTE ability to generate synthetic examples for minority class and ENN ability to delete some observations from both classes that are identified as having different class between the observationâ€™s class and its K-nearest neighbor majority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resampling target\n",
    "resample=SMOTEENN()\n",
    "X_train,Y_train=resample.fit_resample(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Transformation\n",
    "  Feature transformation is a mathematical transformation in which we apply a mathematical formula to data and transform the values which are useful for our further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "powertransformer=PowerTransformer()\n",
    "X_train=powertransformer.fit_transform(X_train)\n",
    "X_test=powertransformer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "            \n",
    "CatBoost is an algorithm for gradient boosting on decision trees. Developed by Yandex researchers and engineers, it is the successor of the MatrixNet algorithm that is widely used within the company for ranking tasks, forecasting and making recommendations\n",
    "\n",
    "#### Tuning parameters\n",
    "\n",
    "1. **learning_rate**:, The learning rate. Used for reducing the gradient step.\n",
    "\n",
    "2. **l2_leaf_reg**: Coefficient at the L2 regularization term of the cost function. Any positive value is allowed.\n",
    "\n",
    "3. **bootstrap_type**: Bootstrap type. Defines the method for sampling the weights of objects.\n",
    "    \n",
    "4. **subsample**: Sample rate for bagging. This parameter can be used if one of the following bootstrap types is selected:\n",
    "\n",
    "For more information refer: [API](https://catboost.ai/en/docs/concepts/python-reference_catboostclassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Initialization\n",
    "model=CatBoostClassifier()\n",
    "model.fit(X_train,Y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Metrics\n",
    " Performance metrics are a part of every machine learning pipeline. They tell you if you're making progress, and put a number on it. All machine learning models,whether it's linear regression, or a SOTA technique like BERT, need a metric to judge performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "plot_confusion_matrix(model,X_test,Y_test,cmap=plt.cm.Blues)\n",
    "# Classification Report\n",
    "print(classification_report(Y_test,model.predict(X_test)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
