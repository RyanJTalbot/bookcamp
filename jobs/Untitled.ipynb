{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136f50bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.indeed.com/jobs?q=data+analyst&l=Remote&sc=0kf%253Aattr(DSQF7)explvl(ENTRY_LEVEL)%253B\n",
    "\n",
    "# https://www.indeed.com/jobs?q=data%20analyst&l=Remote\n",
    "\n",
    "# data+analyst\n",
    "# Remote\n",
    "\n",
    "# https://www.indeed.com/jobs?q=data+analyst&l=Remote&sc=0kf%253Aexplvl(ENTRY_LEVEL)jt(fulltime)%253B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "389f460a",
   "metadata": {},
   "outputs": [],
   "source": [
    "python scraper.py --job_title \"Machine Learning Engineer\" --output_dir data --num_pages 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef85f915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.indeed.com\n"
     ]
    }
   ],
   "source": [
    "headers = {'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.121 Safari/537.36','accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8','referer': 'https://play.google.com/store/apps','accept-language': 'en-US,en;q=0.9,','cookie': 'prov=6bb44cc9-dfe4-1b95-a65d-5250b3b4c9fb; _ga=GA1.2.1363624981.1550767314; __qca=P0-1074700243-1550767314392; notice-ctt=4%3B1550784035760; _gid=GA1.2.1415061800.1552935051; acct=t=4CnQ70qSwPMzOe6jigQlAR28TSW%2fMxzx&s=32zlYt1%2b3TBwWVaCHxH%2bl5aDhLjmq4Xr',}def get_job_urls(URL: str) -> list:'''Extracts job urls from the search result page given by URL'''res = requests.get(URL, headers=headers).contentsoup = BeautifulSoup(res, \"html.parser\")job_urls = [a['href'] for a in soup.find_all('a', {\"id\": lambda x: x and x.startswith('job_')})]return job_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b88bcf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 0\n",
      "Scraping page 10\n",
      "Scraping page 20\n",
      "Scraping page 30\n",
      "Scraping page 40\n",
      "Scrapting Finish! Collected 0 job postings!\n"
     ]
    }
   ],
   "source": [
    "# Create a list to contain all the job postings\n",
    "job_listings = []\n",
    "\n",
    "for page in range(0,500,10): # page from 1 to 100 (last page we can scrape is 100)\n",
    "    if page % 100 == 0:\n",
    "        print(\"Scraping page {}\".format(page // 10))\n",
    "    url = \"%s%s%s%d\" % (base_url, sort_by, start_from, page) # get full url\n",
    "    target = Soup(requests.get(url).text, \"lxml\") \n",
    "\n",
    "    targetElements = target.findAll('div', attrs={'class' : 'result'}) # we're interested in each row (= each job)\n",
    "    \n",
    "    # trying to get each specific job information (such as company name, job title, urls, ...)\n",
    "    for elem in targetElements:\n",
    "        \n",
    "        try:\n",
    "            comp_name = elem.find('span', \"company\").text.strip()\n",
    "            job_title = elem.find('a', attrs={'class':'turnstileLink'}).attrs['title']\n",
    "            job_addr = elem.find('span',\"location\").text\n",
    "            job_link = \"%s%s\" % (home_url,elem.find('a').get('href'))\n",
    "            job_summary = elem.find('span',\"summary\").text.strip()\n",
    "\n",
    "            if elem.find('span', \"company\").find(\"a\"):\n",
    "\n",
    "                company_link = elem.find('span', \"company\").find(\"a\")\n",
    "                comp_link_overall = \"%s%s\" % (home_url, company_link['href'])\n",
    "            else:\n",
    "                comp_link_overall = None\n",
    "\n",
    "            # add a job info to our data frame\n",
    "            job_listings.append({'company_name': comp_name, \n",
    "                                 'job_title': job_title, \n",
    "                                 'job_link': job_link,\n",
    "                                 'job_summary': job_summary,\n",
    "                                 'company_link': comp_link_overall, \n",
    "                                 'job_location': job_addr})\n",
    "        \n",
    "        # Some ofthe listings are missing information, we are going to skip them\n",
    "        except:\n",
    "            print(\"Bad data on search page\")\n",
    "            print(url)\n",
    "            \n",
    "print(\"Scrapting Finish! Collected {} job postings!\".format(len(job_listings)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a1c79c",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
